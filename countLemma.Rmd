Text analysis: Counting lemmata use
========================================================

```{r Copyright}
##    Copyright 2013 Francesco Bailo
    
##    This program is free software: you can redistribute it and/or modify
##    it under the terms of the GNU General Public License as published by
##    the Free Software Foundation, either version 3 of the License, or
##    (at your option) any later version.

##    This program is distributed in the hope that it will be useful,
##    but WITHOUT ANY WARRANTY; without even the implied warranty of
##    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
##    GNU General Public License for more details.

##    You should have received a copy of the GNU General Public License
##    along with this program.  If not, see <http://www.gnu.org/licenses/>.
```    
    

```{r prepareDate, cache=TRUE, warning=FALSE, echo=FALSE}
library(tm)

## SETUP 
# Set collection code
collection_code <- "XX"
# Define language 
project_language <- "it"
# Define input encoding
corpus_encoding <- "UTF-8"
lexicon_encoding <- "latin1"
## 

# Load text file 
# (Replace accordingly with directory containing corpus files and lexicon)
# (See readme file for details on format expected by the code)
source_corpus_dir <- "~/Desktop/Concordanza_Project/texts"
lexicon <- "~/Desktop/Concordanza_Project/morph-it_048.txt"

# Functions
replaceWithLemma <- function (text, dictionary) {
  text_out <- character(0)
  for (i in 1:length(text)) {
    text.split <- strsplit(text[i], "\\s")
    translation <- dictionary[unlist(text.split)]
    text_out <- append(text_out, paste(translation, sep="", collapse=" "))
  }
  PlainTextDocument(text_out, id = ID(text_out), language = Language(text_out))
}

# Create concordance dictionary
lemmata <- read.delim(lexicon, header=F, quote="", encoding = lexicon_encoding)
morph_dict <- as.character(lemmata$V2)
names(morph_dict) <- lemmata$V1
# rm(lemmata, lexicon)

#Load texts
corpus <- Corpus(DirSource(source_corpus_dir, encoding = corpus_encoding), readerControl = list(language = project_language))
# rm(source_corpus_dir)

corpus_raw <- corpus

# Remove stopwords, tolower, remove punctuation
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords(project_language))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, replaceWithLemma, morph_dict)

morph_freq <- t(as.matrix(c(DocumentTermMatrix(corpus))))
morph_freq <- data.frame(morph_freq)
morph_freq[,"Total"] <- rowSums(morph_freq)
morph_freq <- morph_freq[morph_freq$Total>1,]
morph_freq <- morph_freq[order(morph_freq$Total, decreasing=TRUE),]
```

```{r loop, cache=TRUE, warning=FALSE, echo=FALSE}
for (i in 1:nrow(morph_freq)) {
  word <- row.names(morph_freq)[i]
  freq <- morph_freq$Total[i]
  first_line <- paste("## ", toupper(word), " [", freq, "]", sep="", collapse=" ")
  print(first_line)
  
  for (i in 1:length(corpus)) {
    for (j in 1:length(corpus[[i]])) {
      if (grepl(word, corpus[[i]][j])){
        verse <- corpus_raw[[i]][j]
        verse_line <- paste("   ", collection_code, i, j, verse, sep=" ", collapse=" ")
        print(verse_line)
      }
    }
  }
  cat("==========")
}
```